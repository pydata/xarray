{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Merging Data\n",
    "\n",
    "One of xarray's killer features is the ability to merge many individual data files into a single Dataset.\n",
    "This allows scientists to operate at a high mental level, focusing on their scientific questions rather than the details of how the dataset happened to provided.\n",
    "For example, it's common for geospatial data with dimensions *latitude, longitude* to be distributed with one file per day.\n",
    "Xarray allows you to combine all the files into a single Dataset with dimensions *time, latitude, longitdue*.\n",
    "This eliminates the need to loop over the files and process them one by one, as was often done in the past.\n",
    "\n",
    "Xarray tries to combine files automatically via its `open_mfdataset` function. \n",
    "This function examines the file metadata and tries to make reasonable choices about how the user wants the data to be combined.\n",
    "But this doesn't always work right.\n",
    "Xarray can't read your mind.\n",
    "Furthermore, frequently data files have inconsistent or incorrect metadata, or fail to follow established conventions.\n",
    "These are \"dirty\" data.\n",
    "Dirty data can also cause `open_mfdataset` to fail.\n",
    "This is not Xarray's fault.\n",
    "\n",
    "Xarray power users have a range of tricks up their sleeve to overcome these situations.\n",
    "This tutorial explains some strategies for merging dirty data.\n",
    "Understanding these techniques requires a deeper understanding of how Xarray combines data in general.\n",
    "\n",
    "**Note**: This tutorial makes frequent use of python [list comprehensions](#) to iterate over collections concisely. New python users should make sure they are familiar with this code pattern before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "We start with an example of when `open_mfdataset` *does* work right.\n",
    "In this example and all others, we will generate toy data, rather than using real data.\n",
    "Our toy examples has 1x1 deg. lat/lon resolution and one value every 6 months for two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# dataset dimensions\n",
    "ntime, nlat, nlon = 4, 180, 360\n",
    "dims = ['time', 'lat', 'lon']\n",
    "\n",
    "# a dataset with random values but realistic coordinates\n",
    "ds = xr.Dataset({'temperature': (dims, np.random.rand(ntime, nlat, nlon)),\n",
    "                 'pressure': (dims, np.random.rand(ntime, nlat, nlon))},\n",
    "                coords={'time': ('time',\n",
    "                                 pd.date_range('2018-01-01', freq='6MS', periods=ntime)),\n",
    "                        'lat': ('lat', np.arange(-90, 90) + 0.5),\n",
    "                        'lon': ('lon', np.arange(-180, 180) + 0.5)})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write this dataset into 4 distinct files, one per variable per year.\n",
    "We do this by first splitting the dataset into 4 distinct Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times, dsets_temp = zip(*ds[['temperature']].groupby('time.year'))\n",
    "times, dsets_pres = zip(*ds[['pressure']].groupby('time.year'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine one of these individual datasets. Examining datasets is very important for understanding what's happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset just has one 3D array in its data variables.\n",
    "\n",
    "We now write to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf clean_data # just in case you are running this repeatedly\n",
    "\n",
    "import os\n",
    "os.mkdir('clean_data')\n",
    "\n",
    "dsets = dsets_temp + dsets_pres\n",
    "fnames = ([f'clean_data/temperature_{n:02d}.nc' for n in range(len(dsets_temp))] + \n",
    "          [f'clean_data/pressure_{n:02d}.nc' for n in range(len(dsets_pres))])\n",
    "          \n",
    "xr.save_mfdataset(dsets, fnames)\n",
    "!ls clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine just one of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset('clean_data/temperature_00.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is identical the data we generated.\n",
    "We can verify this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_loaded = xr.open_dataset('clean_data/temperature_00.nc')\n",
    "ds_loaded.identical(dsets_temp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `open_mfdataset` do?\n",
    "\n",
    "The aim of the following section is to help de-mystify the `open_mfdataset` function, which is powerful but often a source of user confusion.\n",
    "\n",
    "Let's try to open all the files we just wrote in one go using `open_mfdataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mf = xr.open_mfdataset('clean_data/*.nc')\n",
    "ds_mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything just worked without any special options!\n",
    "üòÅ\n",
    "\n",
    "We got back the same dataset we created back up at the top of this notebook, which we verify by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mf.identical(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we got a nasty warning. üòü\n",
    "\n",
    "This `FutureWarning` tells us that this way of combining data will be deprecated in the future, once Xarray 0.13 is released.\n",
    "Long-time Xarray users should pay close attention to this warning.\n",
    "Code that previously worked may stop working in the future.\n",
    "\n",
    "Let's see what the future behavior will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_mfdataset('clean_data/*.nc', combine='by_coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! üòå It still works.\n",
    "\n",
    "What's happening under the hood in `open_mfdataset`? It's calling the function `combine_by_coords`.\n",
    "We can mimic this behavior by  opening each file individually and then calling that function ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "all_files = glob('clean_data/*.nc')\n",
    "all_dsets = [xr.open_dataset(fname) for fname in all_files]\n",
    "xr.combine_by_coords(all_dsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Dask Chunks\n",
    "\n",
    "You may have noticed that, unlike `open_mfdatset`, the explicit `combine_by_coords` approach above did not produce Dask arrays. Instead, it operated eagerly, loading all the data into memory. This is not what we want with big data. `open_mfdatset` always automatically applies `.chunk()` to the datasets it combines. We can replicate this behavior with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dsets_chunked = [xr.open_dataset(fname, chunks={}) for fname in all_files]\n",
    "xr.combine_by_coords(all_dsets_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also supply a `chunks` keyword to `open_mfdataset` to control chunking more explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_mfdataset('clean_data/*.nc', combine='by_coords', chunks={'time': 1, 'lat': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing is possible with the manual approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dsets_chunked = [xr.open_dataset(fname, chunks={'time': 1, 'lat': 90})\n",
    "                     for fname in all_files]\n",
    "xr.combine_by_coords(all_dsets_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always better to apply chunking in this way, right when you open the individual files, rather than later, after you have already combined files.\n",
    "\n",
    "### More Explicit Manual Combining\n",
    "\n",
    "`combine_by_coords` itself does a few different things under the hood.\n",
    "It uses both `concat` to combine the files along the time dimension and `merge` to combine the two different variables (`temperature` and `pressure`) into a single Dataset. We can do all of these things manually if we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dsets = [xr.open_dataset(fname, chunks={}) for fname in glob('clean_data/temperature_*.nc')]\n",
    "temp_concat = xr.concat(temp_dsets, dim='time')\n",
    "pres_dsets = [xr.open_dataset(fname, chunks={}) for fname in glob('clean_data/pressure_*.nc')]\n",
    "pres_concat = xr.concat(pres_dsets, dim='time')\n",
    "xr.merge([temp_concat, pres_concat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important differences in this manual approach are:\n",
    "\n",
    "1. We had to know in advance that different variables were stored in different files and write some repetitive code. Fortunately this was obvious from the file names, but this is not always the case for real datasets.\n",
    "1. We had to manually specify the `concat_dim` keyword and know in advance that `'time'` was the dimension to concatenate over.\n",
    "1. We had to specify the files in the correct order (more on this below).\n",
    "\n",
    "Manually dataset combining is the most powerful and flexible approach, but, for new, unfamiliar datasets, it requires that you **manually inspect your files carefully!** This is an important general piece of advice, especially once dirty data comes along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Matters for Concatenation!\n",
    "\n",
    "In the example above, we were lucky that `glob('clean_data/temperature_*.nc')` gave us the files in correct chronological order. This is not always guaranteed to be the case, especially if the files follow a weird naming convenion. Let's see what happens if we call `concat` on files in the wrong order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_fnames_wrong_order = ['clean_data/temperature_01.nc', 'clean_data/temperature_00.nc']\n",
    "temp_dsets_wrong_order = [xr.open_dataset(fname, chunks={})\n",
    "                          for fname in temp_fnames_wrong_order]\n",
    "ds_wrong = xr.concat(temp_dsets_wrong_order, dim='time')\n",
    "ds_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Xarray put the data together in the order we provided it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds.time.data, 'o-', label='original')\n",
    "plt.plot(ds_wrong.time.data, '^-', label='wrong time order')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `combine_by_coords` function includes some special logic to try to order the datasets such that the values in their dimension coordinates are monotonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combine_by_coords = xr.combine_by_coords(temp_dsets_wrong_order)\n",
    "\n",
    "plt.plot(ds.time.data, 'o-', label='original')\n",
    "plt.plot(ds_combine_by_coords.time.data, '^-',\n",
    "         label='wrong order but combine_by_coords fixed me')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicitly Enumerate Files\n",
    "\n",
    "The examples above assumed that you wanted to do some wildcard matching (e.g. `*.nc`) to combine files.\n",
    "This is good for exploratory data analysis where you don't know exactly what you're looking for.\n",
    "But for more mature code, or code used in production data processing systems, explicit is better than implicit.\n",
    "If you know the naming conventions that were used to generate your files, you should use this information to explictly specifiy the filenames you want to open. This also has performance implications: `glob` can be very slow on very large directories over some network filesystems.\n",
    "\n",
    "Here is an example of a fully explicit manual combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './clean_data'\n",
    "varnames = ['temperature', 'pressure']\n",
    "time_suffixes = ['00', '01']\n",
    "concat_dim = 'time'\n",
    "\n",
    "variable_dsets = []\n",
    "for vname in varnames:\n",
    "    fnames = [os.path.join(data_dir, f'{vname}_{time_suffix}.nc')\n",
    "              for time_suffix in time_suffixes]\n",
    "    dsets = [xr.open_dataset(fname, chunks={}) for fname in fnames]\n",
    "    ds_concat = xr.concat(dsets, dim=concat_dim)\n",
    "    variable_dsets.append(ds_concat)\n",
    "ds_manually_combined = xr.merge(variable_dsets)\n",
    "ds_manually_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that this is also the same as the original dataset we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_manually_combined.identical(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
